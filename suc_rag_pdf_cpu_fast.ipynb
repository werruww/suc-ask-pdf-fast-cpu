{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdc9FvejnaAL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YR1MrGupe1LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/wangshier108/ComfyUI_RAGDemo/blob/main/test_rag.py"
      ],
      "metadata": {
        "id": "e2P_NMG4m6It"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6DPhsZWe1OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-milvus pymilvus milvus sentence-transformers torch transformers accelerate PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eXTBv2Oe1RJ",
        "outputId": "ed148889-6f4f-47a0-fb58-47b7b2d5c982"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting langchain-milvus\n",
            "  Downloading langchain_milvus-0.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting pymilvus\n",
            "  Downloading pymilvus-2.5.10-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting milvus\n",
            "  Downloading milvus-2.3.5-py3-none-manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (75.2.0)\n",
            "Collecting grpcio<=1.67.1,>=1.49.1 (from pymilvus)\n",
            "  Downloading grpcio-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (5.29.4)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from pymilvus)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ujson>=2.0.0 (from pymilvus)\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (2.2.2)\n",
            "Collecting milvus-lite>=2.4.0 (from pymilvus)\n",
            "  Downloading milvus_lite-2.4.12-py3-none-manylinux2014_x86_64.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_milvus-0.2.0-py3-none-any.whl (35 kB)\n",
            "Downloading pymilvus-2.5.10-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading milvus-2.3.5-py3-none-manylinux2014_x86_64.whl (57.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading grpcio-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading milvus_lite-2.4.12-py3-none-manylinux2014_x86_64.whl (45.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: ujson, python-dotenv, PyPDF2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, milvus-lite, milvus, marshmallow, httpx-sse, grpcio, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pymilvus, pydantic-settings, nvidia-cusolver-cu12, dataclasses-json, langchain-milvus, langchain-community\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.71.0\n",
            "    Uninstalling grpcio-1.71.0:\n",
            "      Successfully uninstalled grpcio-1.71.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires grpcio>=1.71.0, but you have grpcio 1.67.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyPDF2-3.0.1 dataclasses-json-0.6.7 grpcio-1.67.1 httpx-sse-0.4.0 langchain-community-0.3.24 langchain-milvus-0.2.0 marshmallow-3.26.1 milvus-2.3.5 milvus-lite-2.4.12 mypy-extensions-1.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-settings-2.9.1 pymilvus-2.5.10 python-dotenv-1.1.0 typing-inspect-0.9.0 ujson-5.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAARGiEUgdcC",
        "outputId": "483c30f4-054c-4215-cf95-af0a6ce383e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/303.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m225.3/303.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EHh27O8lhKXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python 1.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KzBhdzafDXc",
        "outputId": "69f8e222-9285-4651-9380-0f555035285d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/1.py:8: LangChainDeprecationWarning: Importing HuggingFaceBgeEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
            "usage: 1.py [-h] [--pdf_path PDF_PATH] [--llm_path LLM_PATH]\n",
            "            [--device {cpu,cuda}]\n",
            "            question\n",
            "1.py: error: the following arguments are required: question\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python 1.py \"What is the relationship between Harry and Halley?\""
      ],
      "metadata": {
        "id": "JURzijhzgXAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ApCrSf4HgYGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2R2ovRliKe1",
        "outputId": "76866c47-083e-42a6-e519-a24fe20c2da9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/1.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import argparse # لاستقبال المدخلات من سطر الأوامر\n",
        "\n",
        "# --- استيراد المكتبات الأساسية (نفس ما في الكود الأصلي) ---\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_milvus.vectorstores import Milvus\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- استيراد LLM المخصص ---\n",
        "# افترض أن test_llm.py موجود في نفس المجلد\n",
        "from test_llm import LLaMA3_1_LLM\n",
        "\n",
        "# --- دالة الـ RAG الرئيسية (مأخوذة من دالة test الأصلية مع تعديلات) ---\n",
        "def perform_rag(question: str, pdf_file_path: str, llm_model_path: str, embedding_device: str = 'cpu'):\n",
        "    print(f\"السؤال: {question}\")\n",
        "    print(f\"مسار ملف PDF: {pdf_file_path}\")\n",
        "    print(f\"مسار نموذج LLM: {llm_model_path}\")\n",
        "    print(f\"جهاز التضمين: {embedding_device}\")\n",
        "\n",
        "    if not os.path.exists(pdf_file_path):\n",
        "        print(f\"خطأ: ملف PDF غير موجود في المسار: {pdf_file_path}\")\n",
        "        return \"خطأ: ملف PDF غير موجود.\"\n",
        "\n",
        "    # 1. تحميل ومعالجة PDF\n",
        "    print(\"جارٍ تحميل ومعالجة ملف PDF...\")\n",
        "    loader = PyPDFLoader(file_path=pdf_file_path)\n",
        "    documents = loader.load()\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    print(f\"تم تقسيم المستند إلى {len(docs)} جزء.\")\n",
        "\n",
        "    # 2. إعداد نموذج التضمين\n",
        "    print(\"جارٍ إعداد نموذج التضمين...\")\n",
        "    model_name_embedding = \"BAAI/bge-base-zh-v1.5\"\n",
        "    model_kwargs_embedding = {'device': embedding_device}\n",
        "    encode_kwargs_embedding = {'normalize_embeddings': True}\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name_embedding,\n",
        "        model_kwargs=model_kwargs_embedding,\n",
        "        encode_kwargs=encode_kwargs_embedding,\n",
        "        query_instruction=\"为这个句子生成表示以用于检索相关文章：\"\n",
        "    )\n",
        "    print(\"تم إعداد نموذج التضمين.\")\n",
        "\n",
        "    # 3. إعداد قاعدة البيانات المتجهة (Milvus)\n",
        "    print(\"جارٍ إعداد قاعدة البيانات المتجهة Milvus...\")\n",
        "    # تأكد من أن لديك صلاحيات للكتابة في المجلد الحالي لإنشاء milvus_demo.db\n",
        "    # أو قم بتكوين Milvus للاتصال بخادم إذا كان لديك واحد.\n",
        "    try:\n",
        "        vectorstore = Milvus.from_documents(\n",
        "            documents=docs,\n",
        "            embedding=embedding_model,\n",
        "            collection_name=\"standalone_book_collection\", # اسم مجموعة مختلف لتجنب التعارض\n",
        "            drop_old=True, # احذر: هذا سيحذف المجموعة القديمة بنفس الاسم\n",
        "            connection_args={\"uri\": \"./milvus_standalone_demo.db\"}, # ملف قاعدة بيانات مختلف\n",
        "        )\n",
        "        print(\"تم إعداد Milvus بنجاح.\")\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ أثناء إعداد Milvus: {e}\")\n",
        "        print(\"تأكد من تثبيت مكتبات Milvus (pymilvus, milvus) بشكل صحيح.\")\n",
        "        print(\"إذا كنت تستخدم Milvus server، تأكد أنه يعمل ويمكن الوصول إليه.\")\n",
        "        return \"خطأ في إعداد Milvus.\"\n",
        "\n",
        "    # 4. إعداد الـ Prompt و الـ Retriever\n",
        "    PROMPT_TEMPLATE = \"\"\"\n",
        "    Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\n",
        "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    <context>\n",
        "    {context}\n",
        "    </context>\n",
        "\n",
        "    <question>\n",
        "    {question}\n",
        "    </question>\n",
        "\n",
        "    The response should be specific and use statistics or numbers when possible.\n",
        "\n",
        "    Assistant:\"\"\"\n",
        "    prompt_template = PromptTemplate(\n",
        "        template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    def format_docs(docs_list):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs_list)\n",
        "\n",
        "    # 5. إعداد LLM\n",
        "    print(f\"جارٍ تحميل نموذج LLM من: {llm_model_path}...\")\n",
        "    try:\n",
        "        # افترض أن LLaMA3_1_LLM في test_llm.py يمكنه التعامل مع هذا المسار\n",
        "        llm = LLaMA3_1_LLM(mode_name_or_path=llm_model_path)\n",
        "        print(\"تم تحميل نموذج LLM.\")\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ أثناء تحميل نموذج LLM: {e}\")\n",
        "        print(\"تأكد أن المسار صحيح وأن ملف test_llm.py يمكنه تحميل النموذج من هذا المسار.\")\n",
        "        print(\"قد تحتاج إلى تعديل كود test_llm.py أو توفير مسار محلي للنموذج إذا كان يتوقع ذلك.\")\n",
        "        return \"خطأ في تحميل LLM.\"\n",
        "\n",
        "\n",
        "    # 6. بناء وتشغيل سلسلة RAG\n",
        "    print(\"جارٍ بناء وتشغيل سلسلة RAG...\")\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = rag_chain.invoke(question)\n",
        "        print(\"\\n--- الإجابة ---\")\n",
        "        print(response)\n",
        "        print(\"--- --- ---\")\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ أثناء تشغيل سلسلة RAG: {e}\")\n",
        "        return \"خطأ أثناء توليد الإجابة.\"\n",
        "\n",
        "# --- نقطة الدخول الرئيسية للسكربت ---\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"قم بتشغيل RAG على ملف PDF باستخدام LLM.\")\n",
        "    parser.add_argument(\"question\", type=str, help=\"السؤال الذي تريد طرحه.\")\n",
        "    parser.add_argument(\"--pdf_path\", type=str, default=\"test.pdf\", help=\"المسار إلى ملف PDF (الافتراضي: test.pdf في نفس المجلد).\")\n",
        "    parser.add_argument(\"--llm_path\", type=str, default=\"meta-llama/Llama-3.2-1B-Instruct\", help=\"مسار أو اسم نموذج LLM (الافتراضي: meta-llama/Llama-3.2-1B-Instruct).\")\n",
        "    parser.add_argument(\"--device\", type=str, default=\"cpu\", choices=['cpu', 'cuda'], help=\"الجهاز المستخدم لنموذج التضمين (cpu أو cuda).\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # --- تأكد من وجود ملف test_llm.py ---\n",
        "    if not os.path.exists(\"test_llm.py\"):\n",
        "        print(\"خطأ: ملف test_llm.py غير موجود في نفس المجلد.\")\n",
        "        print(\"هذا الملف ضروري لتحميل النموذج اللغوي الكبير.\")\n",
        "        exit()\n",
        "\n",
        "    # --- تشغيل دالة RAG ---\n",
        "    perform_rag(args.question, args.pdf_path, args.llm_path, args.device)"
      ],
      "metadata": {
        "id": "DlnSZ4KJmgh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/test_llm.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Any, List, Optional\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "class LLaMA3_1_LLM(LLM):\n",
        "    # 基于本地 llama3.1 自定义 LLM 类\n",
        "    tokenizer: AutoTokenizer = None\n",
        "    model: AutoModelForCausalLM = None\n",
        "\n",
        "    def __init__(self, mode_name_or_path :str):\n",
        "\n",
        "        super().__init__()\n",
        "        print(\"正在从本地加载模型...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, use_fast=False)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, torch_dtype=torch.float32, device_map=\"auto\")\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        print(\"完成本地模型的加载\")\n",
        "\n",
        "\n",
        "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
        "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "                **kwargs: Any):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        input_ids = self.tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
        "        model_inputs = self.tokenizer([input_ids], return_tensors=\"pt\").to(self.model.device)\n",
        "        generated_ids = self.model.generate(model_inputs.input_ids,max_new_tokens=512)\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        return response\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"LLaMA3_1_LLM\""
      ],
      "metadata": {
        "id": "iud00_g1mm48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python 1.py \"What is the relationship between Harry and Halley?\" --llm_path \"meta-llama/Llama-3.2-1B-Instruct\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWA8bguUhwkM",
        "outputId": "6d6b58ab-249d-4b26-8064-01df69556ecb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/1.py:8: LangChainDeprecationWarning: Importing HuggingFaceBgeEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
            "السؤال: What is the relationship between Harry and Halley?\n",
            "مسار ملف PDF: test.pdf\n",
            "مسار نموذج LLM: meta-llama/Llama-3.2-1B-Instruct\n",
            "جهاز التضمين: cpu\n",
            "جارٍ تحميل ومعالجة ملف PDF...\n",
            "تم تقسيم المستند إلى 2 جزء.\n",
            "جارٍ إعداد نموذج التضمين...\n",
            "/content/1.py:41: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceBgeEmbeddings(\n",
            "2025-05-30 23:31:20.146392: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748647880.185808    4320 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748647880.197919    4320 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-30 23:31:20.236448: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "تم إعداد نموذج التضمين.\n",
            "جارٍ إعداد قاعدة البيانات المتجهة Milvus...\n",
            "2025-05-30 23:31:33,025 [DEBUG][_create_connection]: Created new connection using: 40147fddcdfa47e3804cfa7f0155cf1e (async_milvus_client.py:599)\n",
            "تم إعداد Milvus بنجاح.\n",
            "جارٍ تحميل نموذج LLM من: meta-llama/Llama-3.2-1B-Instruct...\n",
            "正在从本地加载模型...\n",
            "tokenizer_config.json: 100% 54.5k/54.5k [00:00<00:00, 5.94MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 48.8MB/s]\n",
            "special_tokens_map.json: 100% 296/296 [00:00<00:00, 1.53MB/s]\n",
            "config.json: 100% 877/877 [00:00<00:00, 3.84MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 2.47G/2.47G [00:34<00:00, 71.4MB/s]\n",
            "generation_config.json: 100% 189/189 [00:00<00:00, 1.08MB/s]\n",
            "完成本地模型的加载\n",
            "تم تحميل نموذج LLM.\n",
            "جارٍ بناء وتشغيل سلسلة RAG...\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "\n",
            "--- الإجابة ---\n",
            "根据提供的描述，Harry (哈利) 和 Halley (哈雷) 的关系可以说是亲密的朋友和伴侣。虽然没有提供明确的证据表明他们是夫妻关系，但根据他们的行为和共同的经历，可以推断出他们是 close 的朋友。\n",
            "\n",
            "在村庄中，他们一起经历了多次危险情况，但他们也共同成为了村民们的hero。他们的故事在村庄传为佳话，并且在夜幕降临时，哈雷总是骑上哈利，确保一切安宁。\n",
            "--- --- ---\n",
            "E20250530 23:34:23.003249  4471 server.cpp:47] [SERVER][BlockLock][milvus] Process exit\n",
            "Error in sys.excepthook:\n",
            "\n",
            "Original exception was:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/1.py:8: LangChainDeprecationWarning: Importing HuggingFaceBgeEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
        "\n",
        ">> from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "with new imports of:\n",
        "\n",
        ">> from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
        "  from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "السؤال: What is the relationship between Harry and Halley?\n",
        "مسار ملف PDF: test.pdf\n",
        "مسار نموذج LLM: meta-llama/Llama-3.2-1B-Instruct\n",
        "جهاز التضمين: cpu\n",
        "جارٍ تحميل ومعالجة ملف PDF...\n",
        "تم تقسيم المستند إلى 2 جزء.\n",
        "جارٍ إعداد نموذج التضمين...\n",
        "/content/1.py:41: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
        "  embedding_model = HuggingFaceBgeEmbeddings(\n",
        "2025-05-30 23:31:20.146392: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
        "E0000 00:00:1748647880.185808    4320 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
        "E0000 00:00:1748647880.197919    4320 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
        "2025-05-30 23:31:20.236448: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "تم إعداد نموذج التضمين.\n",
        "جارٍ إعداد قاعدة البيانات المتجهة Milvus...\n",
        "2025-05-30 23:31:33,025 [DEBUG][_create_connection]: Created new connection using: 40147fddcdfa47e3804cfa7f0155cf1e (async_milvus_client.py:599)\n",
        "تم إعداد Milvus بنجاح.\n",
        "جارٍ تحميل نموذج LLM من: meta-llama/Llama-3.2-1B-Instruct...\n",
        "正在从本地加载模型...\n",
        "tokenizer_config.json: 100% 54.5k/54.5k [00:00<00:00, 5.94MB/s]\n",
        "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 48.8MB/s]\n",
        "special_tokens_map.json: 100% 296/296 [00:00<00:00, 1.53MB/s]\n",
        "config.json: 100% 877/877 [00:00<00:00, 3.84MB/s]\n",
        "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
        "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
        "model.safetensors: 100% 2.47G/2.47G [00:34<00:00, 71.4MB/s]\n",
        "generation_config.json: 100% 189/189 [00:00<00:00, 1.08MB/s]\n",
        "完成本地模型的加载\n",
        "تم تحميل نموذج LLM.\n",
        "جارٍ بناء وتشغيل سلسلة RAG...\n",
        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
        "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
        "\n",
        "--- الإجابة ---\n",
        "根据提供的描述，Harry (哈利) 和 Halley (哈雷) 的关系可以说是亲密的朋友和伴侣。虽然没有提供明确的证据表明他们是夫妻关系，但根据他们的行为和共同的经历，可以推断出他们是 close 的朋友。\n",
        "\n",
        "在村庄中，他们一起经历了多次危险情况，但他们也共同成为了村民们的hero。他们的故事在村庄传为佳话，并且在夜幕降临时，哈雷总是骑上哈利，确保一切安宁。\n",
        "--- --- ---\n",
        "E20250530 23:34:23.003249  4471 server.cpp:47] [SERVER][BlockLock][milvus] Process exit\n",
        "Error in sys.excepthook:\n",
        "\n",
        "Original exception was:"
      ],
      "metadata": {
        "id": "v9FZfi9Gh2Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python 1.py \"What are the effects of climate change?\" --llm_path \"meta-llama/Llama-3.2-1B-Instruct\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4oTyaIJkdCt",
        "outputId": "8db25891-7a46-4cce-caf2-6dac81679aeb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/1.py:8: LangChainDeprecationWarning: Importing HuggingFaceBgeEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
            "السؤال: What are the effects of climate change?\n",
            "مسار ملف PDF: test.pdf\n",
            "مسار نموذج LLM: meta-llama/Llama-3.2-1B-Instruct\n",
            "جهاز التضمين: cpu\n",
            "خطأ: ملف PDF غير موجود في المسار: test.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!python rag_standalone_en.py \"Your question?\" --pdf_path \"your_english_book.pdf\" --llm_path \"meta-llama/Llama-3.2-1B-Instruct\" --embedding_model \"sentence-transformers/all-MiniLM-L6-v2\""
      ],
      "metadata": {
        "id": "-4DiL1biltKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/Understanding_Climate_Change.pdf"
      ],
      "metadata": {
        "id": "tPZC0fbZl1Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-o0nBomnfvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/2.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "# --- Core library imports ---\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_milvus.vectorstores import Milvus\n",
        "\n",
        "# Updated import for HuggingFaceBgeEmbeddings\n",
        "# Option 1 (current common practice for langchain_community):\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings as CommunityHuggingFaceEmbeddings\n",
        "# Option 2 (newer, if you install langchain-huggingface):\n",
        "# from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- Custom LLM import ---\n",
        "# Assuming test_llm.py is in the same directory\n",
        "from test_llm import LLaMA3_1_LLM\n",
        "\n",
        "# --- Main RAG function ---\n",
        "def perform_rag(question: str, pdf_file_path: str, llm_model_path: str, embedding_model_name: str, embedding_device: str = 'cpu'):\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"PDF file path: {pdf_file_path}\")\n",
        "    print(f\"LLM model path/name: {llm_model_path}\")\n",
        "    print(f\"Embedding model name: {embedding_model_name}\")\n",
        "    print(f\"Embedding device: {embedding_device}\")\n",
        "\n",
        "    if not os.path.exists(pdf_file_path):\n",
        "        print(f\"Error: PDF file not found at: {pdf_file_path}\")\n",
        "        return \"Error: PDF file not found.\"\n",
        "\n",
        "    # 1. Load and process PDF\n",
        "    print(\"Loading and processing PDF file...\")\n",
        "    loader = PyPDFLoader(file_path=pdf_file_path)\n",
        "    documents = loader.load()\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    print(f\"Document split into {len(docs)} chunks.\")\n",
        "\n",
        "    # 2. Setup embedding model\n",
        "    print(\"Setting up embedding model...\")\n",
        "    # Using CommunityHuggingFaceEmbeddings from langchain_community\n",
        "    # If you installed `langchain-huggingface`, you could use `HuggingFaceEmbeddings` from there.\n",
        "    embedding_model = CommunityHuggingFaceEmbeddings(\n",
        "        model_name=embedding_model_name,\n",
        "        model_kwargs={'device': embedding_device},\n",
        "        encode_kwargs={'normalize_embeddings': True} # Important for cosine similarity\n",
        "    )\n",
        "    # If using BAAI/bge models, they might have a 'query_instruction'\n",
        "    # For 'sentence-transformers/all-MiniLM-L6-v2', it's not typically needed.\n",
        "    # If you switch back to a BGE model, you might add:\n",
        "    # query_instruction=\"Represent this sentence for searching relevant passages:\"\n",
        "    print(\"Embedding model setup complete.\")\n",
        "\n",
        "    # 3. Setup vector store (Milvus)\n",
        "    print(\"Setting up Milvus vector store...\")\n",
        "    try:\n",
        "        vectorstore = Milvus.from_documents(\n",
        "            documents=docs,\n",
        "            embedding=embedding_model,\n",
        "            collection_name=\"rag_standalone_collection_en\", # Different collection name\n",
        "            drop_old=True,\n",
        "            connection_args={\"uri\": \"./milvus_standalone_en.db\"}, # Different DB file\n",
        "        )\n",
        "        print(\"Milvus setup successful.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Milvus setup: {e}\")\n",
        "        print(\"Ensure Milvus libraries (pymilvus, milvus) are correctly installed.\")\n",
        "        print(\"If using a Milvus server, ensure it's running and accessible.\")\n",
        "        return \"Error setting up Milvus.\"\n",
        "\n",
        "    # 4. Setup Prompt and Retriever\n",
        "    PROMPT_TEMPLATE_EN = \"\"\"\n",
        "    Human: You are an AI assistant. Provide concise answers to questions using the\n",
        "    following pieces of context. If you don't know the answer, just say that you don't know.\n",
        "    Do not try to make up an answer.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Assistant:\"\"\"\n",
        "    prompt_template = PromptTemplate(\n",
        "        template=PROMPT_TEMPLATE_EN, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    def format_docs_for_rag(docs_list):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs_list)\n",
        "\n",
        "    # 5. Setup LLM\n",
        "    print(f\"Loading LLM from: {llm_model_path}...\")\n",
        "    try:\n",
        "        llm = LLaMA3_1_LLM(mode_name_or_path=llm_model_path) # Assumes LLaMA3_1_LLM in test_llm.py handles this\n",
        "        print(\"LLM loaded.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading LLM: {e}\")\n",
        "        print(\"Ensure the path is correct and test_llm.py can load the model from this path/name.\")\n",
        "        print(\"You might need to adjust test_llm.py or provide a local model path if it expects one.\")\n",
        "        return \"Error loading LLM.\"\n",
        "\n",
        "    # 6. Build and run RAG chain\n",
        "    print(\"Building and running RAG chain...\")\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs_for_rag, \"question\": RunnablePassthrough()}\n",
        "        | prompt_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = rag_chain.invoke(question)\n",
        "        print(\"\\n--- Answer ---\")\n",
        "        print(response)\n",
        "        print(\"--- --- ---\")\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Error during RAG chain execution: {e}\")\n",
        "        return \"Error generating answer.\"\n",
        "\n",
        "# --- Main script entry point ---\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Run RAG on a PDF file using an LLM.\")\n",
        "    parser.add_argument(\"question\", type=str, help=\"The question to ask.\")\n",
        "    parser.add_argument(\"--pdf_path\", type=str, default=\"english_book.pdf\", help=\"Path to the PDF file (default: english_book.pdf in the same directory).\")\n",
        "    parser.add_argument(\"--llm_path\", type=str, default=\"meta-llama/Llama-3.2-1B-Instruct\", help=\"Path or Hugging Face name of the LLM (default: meta-llama/Llama-3.2-1B-Instruct).\")\n",
        "    parser.add_argument(\"--embedding_model\", type=str, default=\"sentence-transformers/all-MiniLM-L6-v2\", help=\"Hugging Face name of the embedding model (default: sentence-transformers/all-MiniLM-L6-v2).\")\n",
        "    parser.add_argument(\"--device\", type=str, default=\"cpu\", choices=['cpu', 'cuda'], help=\"Device for the embedding model (cpu or cuda).\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(\"test_llm.py\"):\n",
        "        print(\"Error: test_llm.py not found in the current directory.\")\n",
        "        print(\"This file is required to load the Large Language Model.\")\n",
        "        exit()\n",
        "\n",
        "    perform_rag(args.question, args.pdf_path, args.llm_path, args.embedding_model, args.device)"
      ],
      "metadata": {
        "id": "qSbIZ-H_noDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python 2.py \"What are the effects of climate change?\" --pdf_path \"Understanding_Climate_Change.pdf\" --llm_path \"meta-llama/Llama-3.2-1B-Instruct\" --embedding_model \"sentence-transformers/all-MiniLM-L6-v2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEomBOGclv98",
        "outputId": "30e09a35-9b88-451c-fc61-45803e6031b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the effects of climate change?\n",
            "PDF file path: Understanding_Climate_Change.pdf\n",
            "LLM model path/name: meta-llama/Llama-3.2-1B-Instruct\n",
            "Embedding model name: sentence-transformers/all-MiniLM-L6-v2\n",
            "Embedding device: cpu\n",
            "Loading and processing PDF file...\n",
            "Document split into 33 chunks.\n",
            "Setting up embedding model...\n",
            "/content/2.py:47: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = CommunityHuggingFaceEmbeddings(\n",
            "2025-05-30 23:47:44.731334: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748648864.801907    8368 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748648864.821195    8368 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-30 23:47:44.890077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "modules.json: 100% 349/349 [00:00<00:00, 1.23MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 562kB/s]\n",
            "README.md: 100% 10.5k/10.5k [00:00<00:00, 22.6MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 247kB/s]\n",
            "config.json: 100% 612/612 [00:00<00:00, 2.97MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 90.9M/90.9M [00:00<00:00, 115MB/s]\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 1.67MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 5.03MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 25.3MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 605kB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.01MB/s]\n",
            "Embedding model setup complete.\n",
            "Setting up Milvus vector store...\n",
            "2025-05-30 23:48:02,965 [DEBUG][_create_connection]: Created new connection using: 769261a182474fc1957f594ffb4dd44f (async_milvus_client.py:599)\n",
            "Milvus setup successful.\n",
            "Loading LLM from: meta-llama/Llama-3.2-1B-Instruct...\n",
            "正在从本地加载模型...\n",
            "完成本地模型的加载\n",
            "LLM loaded.\n",
            "Building and running RAG chain...\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "\n",
            "--- Answer ---\n",
            "The effects of climate change include:\n",
            "\n",
            "1. Rising global temperatures, leading to more frequent heatwaves, droughts, and heavy rainfall events.\n",
            "2. Increased ice loss, including glacier melting, sea level rise, and changes in ocean currents and weather patterns.\n",
            "3. More frequent and severe droughts, affecting agriculture, water supply, and ecosystems.\n",
            "4. Increased frequency and severity of extreme weather events, such as hurricanes, heatwaves, and floods.\n",
            "5. Ocean acidification, causing harm to marine life, particularly organisms with calcium carbonate shells or skeletons.\n",
            "6. Loss of biodiversity, ecosystem degradation, and decreased availability of natural resources.\n",
            "7. Increased costs of climate-related disasters, such as damage to infrastructure, health care costs, and lost labor productivity.\n",
            "8. Social and environmental inequalities, with marginalized communities bearing the brunt of its impacts.\n",
            "9. Negative impacts on public health, including heat-related illnesses, vector-borne diseases, and respiratory and cardiovascular diseases.\n",
            "10. Loss of ecosystem services, including food security, water security, and climate regulation.\n",
            "--- --- ---\n",
            "E20250530 23:52:57.559772  8515 server.cpp:47] [SERVER][BlockLock][milvus] Process exit\n",
            "Error in sys.excepthook:\n",
            "\n",
            "Original exception was:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pUqtgdjl-vb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}